---
title: "Practical Machine Learning Prediction Assignment"
author: "Kannan Govindarajan"
date: "December 10, 2016"
output: html_document
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
```
## Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, our goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

**Data**

The training data for this project are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

## Introduction

The goal of this project is to predict the manner in which they did the exercise. This is the "classe" (outcome) variable in the training set. We will follow these steps

1. Load the data.
2. Perform exploratory data analysis to understand the training data set.
3. Perform required data conversions such as remove unwanted predictors, remove columns with mostly missing values, remove column which have near zero variance predictors.
4. Split the training data into training data for learning and cross validation
5. Build the prediction model using the algorithm of our choice. 
6. Cross-validate the prediction model. If accuracy needs to be improved, (a) consider using different algorithm for prediction model (b) consider using additional predictional models and ensemble the prediction results.
7. Using finalized prediction model, Predict the outcome variable for the test data set.

## Data Setup

```{r}
# Setup: Load necessary libraries
library(lattice)
library(ggplot2)
library(caret)
library(randomForest)

# Setup: Data 
setwd("C:/Users/Kannan/OneDrive/MyLearning/Coursera/DataScience/Course8") # or any directory of your choice

training <- read.csv("pml-training.csv", na.strings = c("NA", "#DIV/0!", "")) # update missing values with NA
testing  <- read.csv("pml-testing.csv",  na.strings = c("NA", "#DIV/0!", "")) # update missing values with NA

```
## Exploratory Data Analysis

```{r}
dim(training)

table(training$classe)
```
We observe 

1. 19622 rows and 160 variables
2. outcome variable (classe) significantly spreaded along the observations.

## Pre-processsing the data

1. Remove columns that are not needed for analysis
2. Remove columns that have missing values
3. Removing the columns that have  Zero and Near Zero-Variance Predictors

```{r}
# set the seed the for repeatable results
set.seed(12345)
# Remove columns that are not needed for analysis
training <- subset(training,select = -c(X
                                        ,user_name
                                        ,raw_timestamp_part_1
                                        ,raw_timestamp_part_2
                                        ,cvtd_timestamp
                                        ,new_window
                                        ,num_window
                                        )
                   )

testing <- subset(testing,select = -c(X
                                        ,user_name
                                        ,raw_timestamp_part_1
                                        ,raw_timestamp_part_2
                                        ,cvtd_timestamp
                                        ,new_window
                                        ,num_window
                                        )
                   )

# Remove columns that have missing values
isData  <- apply(!is.na(training), 2, sum) >= dim(training)[1]   # number of observations
training <- training[, isData]
testing  <- testing[, isData]

# Removing the columns that have  Zero and Near Zero-Variance Predictors
nZVColumns <- nearZeroVar(training, saveMetrics = TRUE)
training <- training[,nZVColumns$nzv == FALSE]
testing <- testing[,nZVColumns$nzv == FALSE]

```
Here is the final list of variables to be used in building the prediction model.
```{r}
dim(training)

names(training)
```

## Building Prediction Model

Spliting the training data into training data for learning (70%) and cross validation (30%).
```{r}
# split training data into training and validation sets
trainingSplit <- createDataPartition(training$classe, p = 0.70, list = FALSE)
trainLearn <- training[trainingSplit, ]
trainValidation <- training[-trainingSplit, ]
```

Choosing random forest algorithm to build the prediction model, as it is one of the best among classification algorithm to classify large amounts of data with accuracy.
```{r}
# Train the model with learning data set
modelFitRF <- train(classe ~.
                   ,method="rf"
                   ,data=trainLearn
                   ,trControl = trainControl(method="cv", 2)
                   ,ntree=250
                   ,prox=TRUE
                   ,verbose=TRUE
                   ,allowParallel=TRUE
                   )
```

## Cross-validating Prediction Model

```{r}
# Validate the model with the validation data set
predictValidation <- predict(modelFitRF,trainValidation)
# Confusion matrix for validation data to check the accuracy and other measures
confusionMatrix(predictValidation, trainValidation$classe)
```
Accuracy of the prediction model on validation data turns to be 98.98% and Out of sample error rate is 1.02%.
```{r}
# Calculate Out of Sample Error rate
missClass = function(values, predicted) {
  sum(predicted != values) / length(values)
}
outOfSampleErrorRate = missClass(trainValidation$classe, predictValidation)
outOfSampleErrorRate
```
Below are the most important predictors from the prediction model.
```{r}
# most important variables from the model
importVariables <- varImp(modelFitRF, scale=FALSE)
plot(importVariables, top = 20)
```

## Predicting outcome for test data set
```{r}
# Predict the outcome for test data set
predictTest <- predict(modelFitRF, newdata = testing)
testing$classe <- predictTest
predictTest
```
